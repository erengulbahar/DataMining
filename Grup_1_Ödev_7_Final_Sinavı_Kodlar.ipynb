{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39105ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import string\n",
    "from zemberek import TurkishSpellChecker, TurkishSentenceNormalizer, TurkishSentenceExtractor, TurkishMorphology, TurkishTokenizer\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b3af97",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4354f304",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ERENGULBAHAR\\AppData\\Local\\Temp\\ipykernel_22144\\2950316622.py:54: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  tweets = tweets.str.replace(\"\\d+\", \"\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-02 15:26:36,604 - zemberek.morphology.turkish_morphology - INFO\n",
      "Msg: TurkishMorphology instance initialized in 7.066076278686523\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "#Read Turkish stopwords\n",
    "my_file = open(\"stopwords.txt\",\"r\", encoding=\"UTF-8\")\n",
    "turkish_stopwords = my_file.read()\n",
    "turkish_stopwords = turkish_stopwords.replace(\"\\n\", \" \").split(\" \")\n",
    "\n",
    "#Read Excel file\n",
    "twitter_analiz = pd.read_excel(\"Grup1_Etiketleme.xlsx\")\n",
    "df = twitter_analiz.copy()\n",
    "tweets = df[\"Tweet\"]\n",
    "\n",
    "#Clear all emojis\n",
    "def remove_emoji(string):\n",
    "    emoj = re.compile(\"[\"\n",
    "                      u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                      u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                      u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                      u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                      u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                      u\"\\U00002702-\\U000027B0\"\n",
    "                      u\"\\U00002702-\\U000027B0\"\n",
    "                      u\"\\U000024C2-\\U0001F251\"\n",
    "                      u\"\\U0001f926-\\U0001f937\"\n",
    "                      u\"\\U00010000-\\U0010ffff\"\n",
    "                      u\"\\u2640-\\u2642\"\n",
    "                      u\"\\u2600-\\u2B55\"\n",
    "                      u\"\\u200d\"\n",
    "                      u\"\\u23cf\"\n",
    "                      u\"\\u23e9\"\n",
    "                      u\"\\u231a\"\n",
    "                      u\"\\ufe0f\"  # dingbats\n",
    "                      u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "\n",
    "    return re.sub(emoj, '', string)\n",
    "\n",
    "#Clear all links\n",
    "def remove_urls(text):\n",
    "    result = re.sub(r'http\\S+', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    return result\n",
    "\n",
    "def remove_hastaghes(text):\n",
    "    result = re.sub(r'#\\S+', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    return result\n",
    "\n",
    "#Make lower case\n",
    "tweets = tweets.apply(lambda x: str(x).lower())\n",
    "#Delete all punctuation\n",
    "tweets = tweets.apply(lambda x: str(x).translate(str.maketrans(\"\", \"\", string.punctuation)))\n",
    "#Delete all numbers from string\n",
    "tweets = tweets.str.replace(\"\\d+\", \"\")\n",
    "#Remove all emojis\n",
    "tweets = tweets.apply(lambda x: remove_emoji(x))\n",
    "#Remove all URLs\n",
    "tweets = tweets.apply(lambda x: remove_urls(x))\n",
    "#Remove all hastaghes\n",
    "tweets = tweets.apply(lambda x: remove_hastaghes(x))\n",
    "#Delete all Turkish stopwords\n",
    "tweets = tweets.apply(lambda x: x if x not in turkish_stopwords else None)\n",
    "\n",
    "#Make sentence\n",
    "str_tweets = \"\"\n",
    "\n",
    "for i in range(len(tweets)):\n",
    "    str_tweets += str(tweets[i]).replace(\"\\n\", \"\") + \" \"\n",
    "\n",
    "#Create morphology and normalizer objects\n",
    "morphology = TurkishMorphology.create_with_defaults()\n",
    "normalizer = TurkishSentenceNormalizer(morphology)\n",
    "\n",
    "analysis_words = morphology.analyze_sentence(str_tweets)\n",
    "disambiguate_words = morphology.disambiguate(str_tweets, analysis_words)\n",
    "\n",
    "#Make series\n",
    "tweets_word_roots = \"\"\n",
    "\n",
    "for i in disambiguate_words.best_analysis():\n",
    "    if(str(i.item.root) != \"UNK\"):\n",
    "        tweets_word_roots += str(i.item.root) + \" \"\n",
    "\n",
    "#Splitting by words\n",
    "tweets_word_roots = tweets_word_roots.split(\" \")\n",
    "#Make Pandas Series\n",
    "tweets_word_roots_series = pd.Series(tweets_word_roots)\n",
    "#Make Pandas DataFrame\n",
    "tweets_words_root_df = pd.DataFrame(tweets_word_roots_series, columns=[\"Root\"])\n",
    "\n",
    "#Saving DataFrame as Excel file\n",
    "tweets_words_root_df.to_excel(\"tweets_words_roots.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfd37f4",
   "metadata": {},
   "source": [
    "# Concat tweets and have been most frequency roots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45dc7a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from zemberek import TurkishSentenceNormalizer, TurkishMorphology\n",
    "from sklearn.neighbors import NearestNeighbors, KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "242cf8f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          kargo\n",
       "1          kadın\n",
       "2             bu\n",
       "3            tür\n",
       "4           foto\n",
       "          ...   \n",
       "2975    resmiyet\n",
       "2976       doğma\n",
       "2977     gerekli\n",
       "2978      hamile\n",
       "2979         düz\n",
       "Name: UniqueRoot, Length: 2980, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get uniqe roots\n",
    "uniqeRoots = pd.read_excel(\"tweets_words_roots.xlsx\");\n",
    "uniqeRoots = uniqeRoots[\"UniqueRoot\"]\n",
    "uniqeRoots.dropna(inplace=True)\n",
    "uniqeRoots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "227b8097",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                   @labirsev Kargo kadın\n",
       "1       @ElfesyaSeher Bu tür fotoların yayınlanıp olağ...\n",
       "2       beni çocukluğuma götüren kokulardan birisi pla...\n",
       "3                                         Ana valla sapık\n",
       "4                   @NoContextSag Sapik Anadolu muslumani\n",
       "                              ...                        \n",
       "996     Yaratılışa aykırı olan her düşünce yaşadığımız...\n",
       "997     Leonarda da Vinci eşcinsel ve veganmış. Çağımı...\n",
       "998     Ahmet YILDIZ'A Adalet! , 15 Temmuz 2008 tarihi...\n",
       "999     Ailede birinin eşcinsel doğması gerekliydi ve ...\n",
       "1000    @jeandpardaillan El hareketi günümüzde kabul e...\n",
       "Name: Tweet, Length: 1001, dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get tweets\n",
    "tweets = pd.read_excel(\"Grup1_Etiketleme.xlsx\")\n",
    "tweets = tweets[\"Tweet\"]\n",
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0d94546e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ERENGULBAHAR\\AppData\\Local\\Temp\\ipykernel_22144\\2160291558.py:54: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  tweets = tweets.str.replace(\"\\d+\", \"\")\n"
     ]
    }
   ],
   "source": [
    "#We saving tweets as like cleaning from unnecessary things\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "#Read Turkish stopwords\n",
    "my_file = open(\"stopwords.txt\",\"r\", encoding=\"UTF-8\")\n",
    "turkish_stopwords = my_file.read()\n",
    "turkish_stopwords = turkish_stopwords.replace(\"\\n\", \" \").split(\" \")\n",
    "\n",
    "#Read Excel file\n",
    "twitter_analiz = pd.read_excel(\"Grup1_Etiketleme.xlsx\")\n",
    "df = twitter_analiz.copy()\n",
    "tweets = df[\"Tweet\"]\n",
    "\n",
    "#Clear all emojis\n",
    "def remove_emoji(string):\n",
    "    emoj = re.compile(\"[\"\n",
    "                      u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                      u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                      u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                      u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                      u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                      u\"\\U00002702-\\U000027B0\"\n",
    "                      u\"\\U00002702-\\U000027B0\"\n",
    "                      u\"\\U000024C2-\\U0001F251\"\n",
    "                      u\"\\U0001f926-\\U0001f937\"\n",
    "                      u\"\\U00010000-\\U0010ffff\"\n",
    "                      u\"\\u2640-\\u2642\"\n",
    "                      u\"\\u2600-\\u2B55\"\n",
    "                      u\"\\u200d\"\n",
    "                      u\"\\u23cf\"\n",
    "                      u\"\\u23e9\"\n",
    "                      u\"\\u231a\"\n",
    "                      u\"\\ufe0f\"  # dingbats\n",
    "                      u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "\n",
    "    return re.sub(emoj, '', string)\n",
    "\n",
    "#Clear all links\n",
    "def remove_urls(text):\n",
    "    result = re.sub(r'http\\S+', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    return result\n",
    "\n",
    "def remove_hastaghes(text):\n",
    "    result = re.sub(r'#\\S+', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    return result\n",
    "\n",
    "#Make lower case\n",
    "tweets = tweets.apply(lambda x: str(x).lower())\n",
    "#Delete all punctuation\n",
    "tweets = tweets.apply(lambda x: str(x).translate(str.maketrans(\"\", \"\", string.punctuation)))\n",
    "#Delete all numbers from string\n",
    "tweets = tweets.str.replace(\"\\d+\", \"\")\n",
    "#Remove all emojis\n",
    "tweets = tweets.apply(lambda x: remove_emoji(x))\n",
    "#Remove all URLs\n",
    "tweets = tweets.apply(lambda x: remove_urls(x))\n",
    "#Remove all hastaghes\n",
    "tweets = tweets.apply(lambda x: remove_hastaghes(x))\n",
    "#Delete all Turkish stopwords\n",
    "tweets = tweets.apply(lambda x: x if x not in turkish_stopwords else None)\n",
    "\n",
    "tweets.to_csv(\"cleaned_tweets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1e521d71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                    labirsev kargo kadın\n",
       "1       elfesyaseher bu tür fotoların yayınlanıp olağa...\n",
       "2       beni çocukluğuma götüren kokulardan birisi pla...\n",
       "3                                         ana valla sapık\n",
       "4                    nocontextsag sapik anadolu muslumani\n",
       "                              ...                        \n",
       "996     yaratılışa aykırı olan her düşünce yaşadığımız...\n",
       "997     leonarda da vinci eşcinsel ve veganmış çağımız...\n",
       "998     ahmet yildiza adalet   temmuz  tarihinde babas...\n",
       "999     ailede birinin eşcinsel doğması gerekliydi ve ...\n",
       "1000    jeandpardaillan el hareketi günümüzde kabul ed...\n",
       "Name: Tweet, Length: 1001, dtype: object"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We create list by roots\n",
    "columns = list(uniqeRoots)\n",
    "\n",
    "#We get cleaned tweets\n",
    "cleaned_tweets = pd.read_csv(\"cleaned_tweets.csv\")\n",
    "cleaned_tweets = cleaned_tweets[\"Tweet\"]\n",
    "cleaned_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "dd4be0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read Excel file that has all roots\n",
    "df = pd.read_excel(\"tweets_words_roots.xlsx\")\n",
    "df = df[\"Root\"]\n",
    "\n",
    "#We calculate frequency each of words\n",
    "information_gain = df.value_counts(normalize=True)\n",
    "\n",
    "#We make dictionary structure for words and frequencies\n",
    "temp_dictionary = dict(information_gain)\n",
    "most_words = list(temp_dictionary.keys())\n",
    "temp_list = list()\n",
    "\n",
    "for i in range(1010):\n",
    "    temp_list.append(most_words[i])\n",
    "\n",
    "best_words = pd.Series(temp_list)\n",
    "\n",
    "best_words.to_excel(\"most_roots.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "8c8f8f78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0             ol\n",
       "1       eşcinsel\n",
       "2          sapık\n",
       "3             bu\n",
       "4             de\n",
       "          ...   \n",
       "996         alev\n",
       "997        zafer\n",
       "998        yaşam\n",
       "999        oktar\n",
       "1000       ciddi\n",
       "Name: Roots, Length: 1001, dtype: object"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We get have most frequency roots\n",
    "most_roots = pd.read_excel(\"most_roots.xlsx\")\n",
    "most_roots = most_roots[\"Roots\"]\n",
    "most_roots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cf1b98b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>Roots</th>\n",
       "      <th>ol</th>\n",
       "      <th>eşcinsel</th>\n",
       "      <th>sapık</th>\n",
       "      <th>bu</th>\n",
       "      <th>de</th>\n",
       "      <th>bir</th>\n",
       "      <th>ve</th>\n",
       "      <th>et</th>\n",
       "      <th>ben</th>\n",
       "      <th>yap</th>\n",
       "      <th>...</th>\n",
       "      <th>aşağı</th>\n",
       "      <th>dikkat</th>\n",
       "      <th>cuma</th>\n",
       "      <th>yasak</th>\n",
       "      <th>imkan</th>\n",
       "      <th>mezhep</th>\n",
       "      <th>alev</th>\n",
       "      <th>zafer</th>\n",
       "      <th>yaşam</th>\n",
       "      <th>oktar</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tweet</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>@labirsev Kargo kadın</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>@ElfesyaSeher Bu tür fotoların yayınlanıp olağanlaştırılmasına,cinsel hayata saygısızlığa karşıyım…eminim ki sapık papazların da bir çok fotosu vardır da ben henüz rastlamadım..din kutsaldır;kirletilmemeli …</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beni çocukluğuma götüren kokulardan birisi playstation 1 kokusu ve yeni macbook prolar aynen öyle kokuyor. sapık gibi koklayıp mutlu oluyorum.</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ana valla sapık</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>@NoContextSag Sapik Anadolu muslumani</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Yaratılışa aykırı olan her düşünce yaşadığımız yüzyıla hakim olan sistemin bir ürünüdür. Yaradan insanı kadın ve erkek olarak, hem etçil hem de otçul yaratmıştır. Doğuştan eşcinsel olunmayacağı gibi doğuştan vegan da olunmaz. İnsanoğlu doğal dengeden tamamen koparılmak isteniyor.</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Leonarda da Vinci eşcinsel ve veganmış. Çağımızda yaşasaydı twitterda neler yazardı çok merak ediyorum.</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ahmet YILDIZ'A Adalet! , 15 Temmuz 2008 tarihinde babası tarafından Üsküdar’da “sırf” eşcinsel olduğu için katledilmiştir! 8 Eylül 2009 tarihinde açılan Ahmet YILDIZ cinayeti davası, Türkiye’de ilk defa eşcinsel namus cinayeti kavramını resmiyete taşıdı. https://t.co/iHx1H30KrE</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ailede birinin eşcinsel doğması gerekliydi ve o görevi ben üstlendim</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>@jeandpardaillan El hareketi günümüzde kabul edildiği gibi hamileliği temsil ediyor olabilir.Ensest ilişki anlamına geldiğini de duymuştum ya da düz bakacak olursak eşcinsel ilişkiyi de anlatıyor olabilir</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1001 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Roots                                              ol eşcinsel sapık bu de  \\\n",
       "Tweet                                                                        \n",
       "@labirsev Kargo kadın                               0        0     0  0  0   \n",
       "@ElfesyaSeher Bu tür fotoların yayınlanıp olağa...  0        0     0  0  0   \n",
       "beni çocukluğuma götüren kokulardan birisi play...  0        0     0  0  0   \n",
       "Ana valla sapık                                     0        0     0  0  0   \n",
       "@NoContextSag Sapik Anadolu muslumani               0        0     0  0  0   \n",
       "...                                                ..      ...   ... .. ..   \n",
       "Yaratılışa aykırı olan her düşünce yaşadığımız ...  0        0     0  0  0   \n",
       "Leonarda da Vinci eşcinsel ve veganmış. Çağımız...  0        0     0  0  0   \n",
       "Ahmet YILDIZ'A Adalet! , 15 Temmuz 2008 tarihin...  0        0     0  0  0   \n",
       "Ailede birinin eşcinsel doğması gerekliydi ve o...  0        0     0  0  0   \n",
       "@jeandpardaillan El hareketi günümüzde kabul ed...  0        0     0  0  0   \n",
       "\n",
       "Roots                                              bir ve et ben yap  ...  \\\n",
       "Tweet                                                                 ...   \n",
       "@labirsev Kargo kadın                                0  0  0   0   0  ...   \n",
       "@ElfesyaSeher Bu tür fotoların yayınlanıp olağa...   0  0  0   0   0  ...   \n",
       "beni çocukluğuma götüren kokulardan birisi play...   0  0  0   0   0  ...   \n",
       "Ana valla sapık                                      0  0  0   0   0  ...   \n",
       "@NoContextSag Sapik Anadolu muslumani                0  0  0   0   0  ...   \n",
       "...                                                 .. .. ..  ..  ..  ...   \n",
       "Yaratılışa aykırı olan her düşünce yaşadığımız ...   0  0  0   0   0  ...   \n",
       "Leonarda da Vinci eşcinsel ve veganmış. Çağımız...   0  0  0   0   0  ...   \n",
       "Ahmet YILDIZ'A Adalet! , 15 Temmuz 2008 tarihin...   0  0  0   0   0  ...   \n",
       "Ailede birinin eşcinsel doğması gerekliydi ve o...   0  0  0   0   0  ...   \n",
       "@jeandpardaillan El hareketi günümüzde kabul ed...   0  0  0   0   0  ...   \n",
       "\n",
       "Roots                                              aşağı dikkat cuma yasak  \\\n",
       "Tweet                                                                        \n",
       "@labirsev Kargo kadın                                  0      0    0     0   \n",
       "@ElfesyaSeher Bu tür fotoların yayınlanıp olağa...     0      0    0     0   \n",
       "beni çocukluğuma götüren kokulardan birisi play...     0      0    0     0   \n",
       "Ana valla sapık                                        0      0    0     0   \n",
       "@NoContextSag Sapik Anadolu muslumani                  0      0    0     0   \n",
       "...                                                  ...    ...  ...   ...   \n",
       "Yaratılışa aykırı olan her düşünce yaşadığımız ...     0      0    0     0   \n",
       "Leonarda da Vinci eşcinsel ve veganmış. Çağımız...     0      0    0     0   \n",
       "Ahmet YILDIZ'A Adalet! , 15 Temmuz 2008 tarihin...     0      0    0     0   \n",
       "Ailede birinin eşcinsel doğması gerekliydi ve o...     0      0    0     0   \n",
       "@jeandpardaillan El hareketi günümüzde kabul ed...     0      0    0     0   \n",
       "\n",
       "Roots                                              imkan mezhep alev zafer  \\\n",
       "Tweet                                                                        \n",
       "@labirsev Kargo kadın                                  0      0    0     0   \n",
       "@ElfesyaSeher Bu tür fotoların yayınlanıp olağa...     0      0    0     0   \n",
       "beni çocukluğuma götüren kokulardan birisi play...     0      0    0     0   \n",
       "Ana valla sapık                                        0      0    0     0   \n",
       "@NoContextSag Sapik Anadolu muslumani                  0      0    0     0   \n",
       "...                                                  ...    ...  ...   ...   \n",
       "Yaratılışa aykırı olan her düşünce yaşadığımız ...     0      0    0     0   \n",
       "Leonarda da Vinci eşcinsel ve veganmış. Çağımız...     0      0    0     0   \n",
       "Ahmet YILDIZ'A Adalet! , 15 Temmuz 2008 tarihin...     0      0    0     0   \n",
       "Ailede birinin eşcinsel doğması gerekliydi ve o...     0      0    0     0   \n",
       "@jeandpardaillan El hareketi günümüzde kabul ed...     0      0    0     0   \n",
       "\n",
       "Roots                                              yaşam oktar  \n",
       "Tweet                                                           \n",
       "@labirsev Kargo kadın                                  0     0  \n",
       "@ElfesyaSeher Bu tür fotoların yayınlanıp olağa...     0     0  \n",
       "beni çocukluğuma götüren kokulardan birisi play...     0     0  \n",
       "Ana valla sapık                                        0     0  \n",
       "@NoContextSag Sapik Anadolu muslumani                  0     0  \n",
       "...                                                  ...   ...  \n",
       "Yaratılışa aykırı olan her düşünce yaşadığımız ...     0     0  \n",
       "Leonarda da Vinci eşcinsel ve veganmış. Çağımız...     0     0  \n",
       "Ahmet YILDIZ'A Adalet! , 15 Temmuz 2008 tarihin...     0     0  \n",
       "Ailede birinin eşcinsel doğması gerekliydi ve o...     0     0  \n",
       "@jeandpardaillan El hareketi günümüzde kabul ed...     0     0  \n",
       "\n",
       "[1001 rows x 1000 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create new excel file including unique roots and all tweets\n",
    "tweets_matrix = pd.DataFrame(columns=[most_roots], index=[tweets])\n",
    "tweets_matrix.fillna(0, inplace=True)\n",
    "tweets_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "a061ec85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                    labirsev kargo kadın\n",
       "1       elfesyaseher bu tür fotoların yayınlanıp olağa...\n",
       "2       beni çocukluğuma götüren kokulardan birisi pla...\n",
       "3                                         ana valla sapık\n",
       "4                    nocontextsag sapik anadolu muslumani\n",
       "                              ...                        \n",
       "996     yaratılışa aykırı olan her düşünce yaşadığımız...\n",
       "997     leonarda da vinci eşcinsel ve veganmış çağımız...\n",
       "998     ahmet yildiza adalet   temmuz  tarihinde babas...\n",
       "999     ailede birinin eşcinsel doğması gerekliydi ve ...\n",
       "1000    jeandpardaillan el hareketi günümüzde kabul ed...\n",
       "Name: Tweet, Length: 1001, dtype: object"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c05cda7",
   "metadata": {},
   "source": [
    "# Create frequency matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "922741aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-02 17:03:57,083 - zemberek.morphology.turkish_morphology - INFO\n",
      "Msg: TurkishMorphology instance initialized in 7.008368968963623\n",
      "\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "998\n",
      "999\n",
      "1000\n",
      "1001\n"
     ]
    }
   ],
   "source": [
    "#We create frequency matrix\n",
    "morphology = TurkishMorphology.create_with_defaults()\n",
    "normalizer = TurkishSentenceNormalizer(morphology)\n",
    "\n",
    "counter = 0\n",
    "\n",
    "for i in range(len(cleaned_tweets)):\n",
    "    analysis_words = morphology.analyze_sentence(str(cleaned_tweets[i]))\n",
    "    disambiguate_words = morphology.disambiguate(str(cleaned_tweets[i]), analysis_words)\n",
    "    tweets_word_roots = \"\"\n",
    "\n",
    "    for j in disambiguate_words.best_analysis():\n",
    "        if (str(j.item.root) != \"UNK\"):\n",
    "            tweets_word_roots += str(j.item.root) + \" \"\n",
    "\n",
    "    temp_roots_list = tweets_word_roots.split(\" \")\n",
    "\n",
    "    matrix = [0] * (len(most_roots))\n",
    "    \n",
    "    for k in range(len(most_roots)):\n",
    "        value = 0\n",
    "        \n",
    "        for l in range(len(temp_roots_list)):\n",
    "            if(temp_roots_list[l] == most_roots[k]):\n",
    "                value += 1\n",
    "                matrix[k] = value\n",
    "\n",
    "    tweets_matrix.iloc[i:i+1, :] = matrix\n",
    "\n",
    "    counter += 1\n",
    "\n",
    "    print(counter)\n",
    "\n",
    "tweets_matrix.to_csv(\"tweets_matrix.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20caf128",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_matrix = pd.read_excel(\"tweets_matrix_excel.xlsx\")\n",
    "tweets_matrix_columns = tweets_matrix.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "379dc9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tweets_matrix.loc[:, tweets_matrix_columns[1:]]\n",
    "Y = tweets_matrix.loc[:, [\"Tweets\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fc611ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e7b1f1",
   "metadata": {},
   "source": [
    "# KNN Cosine Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68064a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We do standartization\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=1, algorithm=\"brute\", metric=\"cosine\")\n",
    "knn.fit(X_train, Y_train)\n",
    "\n",
    "#We predict with our model\n",
    "Y_pred = knn.predict(X_test)\n",
    "#We calculate accuracy\n",
    "accuracy = accuracy_score(Y_test, Y_pred)\n",
    "#We calculate f1 score\n",
    "f1_scores = f1_score(Y_test, Y_pred, average=\"weighted\")\n",
    "\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"F1 Score (Weighted): \", f1_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144481f1",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "e65080e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-06 04:07:03,082 - zemberek.morphology.turkish_morphology - INFO\n",
      "Msg: TurkishMorphology instance initialized in 6.691812992095947\n",
      "\n"
     ]
    }
   ],
   "source": [
    "morphology = TurkishMorphology.create_with_defaults()\n",
    "normalizer = TurkishSentenceNormalizer(morphology)\n",
    "\n",
    "tweets_list = list()\n",
    "\n",
    "for i in range(len(cleaned_tweets)):\n",
    "    analysis_words = morphology.analyze_sentence(str(cleaned_tweets[i]))\n",
    "    disambiguate_words = morphology.disambiguate(str(cleaned_tweets[i]), analysis_words)\n",
    "    tweets_word_roots = \"\"\n",
    "\n",
    "    for j in disambiguate_words.best_analysis():\n",
    "        if (str(j.item.root) != \"UNK\"):\n",
    "            tweets_word_roots += str(j.item.root) + \" \"\n",
    "    \n",
    "    tweets_list.append(tweets_word_roots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "3d2a9f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "c22bc243",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_model = TfidfVectorizer()\n",
    "tf_idf_vector = tf_idf_model.fit_transform(tweets_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "e8093aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We take tf-idf values\n",
    "tf_idf_array = tf_idf_vector.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "6d933fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We take roots\n",
    "tf_idf_roots = list(tf_idf_model.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "4eb717b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df_tf_idf = pd.DataFrame(tf_idf_array, columns=tf_idf_roots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "087408eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tf_idf = pd.read_excel(\"temp_cleaned_tweets.xlsx\")\n",
    "df_tf_idf.columns = most_roots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "56269856",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tf_idf.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "430c725d",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_words_list = list(most_roots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "24958a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We create new dataframe\n",
    "for i in range(count_roots):\n",
    "    df_tf_idf[most_roots[i]] = temp_df_tf_idf[most_roots[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "c8d06fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tf_idf.insert(0, \"Tweets\", cleaned_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "8af94bbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Roots</th>\n",
       "      <th>Tweets</th>\n",
       "      <th>ol</th>\n",
       "      <th>eşcinsel</th>\n",
       "      <th>sapık</th>\n",
       "      <th>bu</th>\n",
       "      <th>de</th>\n",
       "      <th>bir</th>\n",
       "      <th>ve</th>\n",
       "      <th>et</th>\n",
       "      <th>ben</th>\n",
       "      <th>...</th>\n",
       "      <th>dikkat</th>\n",
       "      <th>cuma</th>\n",
       "      <th>yasak</th>\n",
       "      <th>imkan</th>\n",
       "      <th>mezhep</th>\n",
       "      <th>alev</th>\n",
       "      <th>zafer</th>\n",
       "      <th>yaşam</th>\n",
       "      <th>oktar</th>\n",
       "      <th>ciddi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>labirsev kargo kadın</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>elfesyaseher bu tür fotoların yayınlanıp olağa...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.081884</td>\n",
       "      <td>0.092466</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.106136</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.128971</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>beni çocukluğuma götüren kokulardan birisi pla...</td>\n",
       "      <td>0.071309</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.069566</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.101478</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.109569</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ana valla sapık</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.231455</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nocontextsag sapik anadolu muslumani</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1002 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Roots                                             Tweets        ol  eşcinsel  \\\n",
       "0                                   labirsev kargo kadın  0.000000       0.0   \n",
       "1      elfesyaseher bu tür fotoların yayınlanıp olağa...  0.000000       0.0   \n",
       "2      beni çocukluğuma götüren kokulardan birisi pla...  0.071309       0.0   \n",
       "3                                        ana valla sapık  0.000000       0.0   \n",
       "4                   nocontextsag sapik anadolu muslumani  0.000000       0.0   \n",
       "\n",
       "Roots     sapık        bu   de       bir        ve   et       ben  ...  \\\n",
       "0      0.000000  0.000000  0.0  0.000000  0.000000  0.0  0.000000  ...   \n",
       "1      0.081884  0.092466  0.0  0.106136  0.000000  0.0  0.128971  ...   \n",
       "2      0.069566  0.000000  0.0  0.000000  0.101478  0.0  0.109569  ...   \n",
       "3      0.231455  0.000000  0.0  0.000000  0.000000  0.0  0.000000  ...   \n",
       "4      0.000000  0.000000  0.0  0.000000  0.000000  0.0  0.000000  ...   \n",
       "\n",
       "Roots  dikkat  cuma  yasak  imkan  mezhep  alev  zafer  yaşam  oktar  ciddi  \n",
       "0         0.0   0.0    0.0    0.0     0.0   0.0    0.0    0.0    0.0    0.0  \n",
       "1         0.0   0.0    0.0    0.0     0.0   0.0    0.0    0.0    0.0    0.0  \n",
       "2         0.0   0.0    0.0    0.0     0.0   0.0    0.0    0.0    0.0    0.0  \n",
       "3         0.0   0.0    0.0    0.0     0.0   0.0    0.0    0.0    0.0    0.0  \n",
       "4         0.0   0.0    0.0    0.0     0.0   0.0    0.0    0.0    0.0    0.0  \n",
       "\n",
       "[5 rows x 1002 columns]"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tf_idf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "5d3932cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tf_idf.to_excel(\"tf_idf_matrix.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5815afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read TF-IDF tweets matrix\n",
    "df = pd.read_excel(\"tf_idf_matrix.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c0bf304",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "      <th>ol</th>\n",
       "      <th>eşcinsel</th>\n",
       "      <th>sapık</th>\n",
       "      <th>bu</th>\n",
       "      <th>de</th>\n",
       "      <th>bir</th>\n",
       "      <th>ve</th>\n",
       "      <th>et</th>\n",
       "      <th>ben</th>\n",
       "      <th>...</th>\n",
       "      <th>dikkat</th>\n",
       "      <th>cuma</th>\n",
       "      <th>yasak</th>\n",
       "      <th>imkan</th>\n",
       "      <th>mezhep</th>\n",
       "      <th>alev</th>\n",
       "      <th>zafer</th>\n",
       "      <th>yaşam</th>\n",
       "      <th>oktar</th>\n",
       "      <th>ciddi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>labirsev kargo kadın</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>elfesyaseher bu tür fotoların yayınlanıp olağa...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081884</td>\n",
       "      <td>0.092466</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.106136</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.128971</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>beni çocukluğuma götüren kokulardan birisi pla...</td>\n",
       "      <td>0.071309</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.069566</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.101478</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.109569</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ana valla sapık</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.231455</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nocontextsag sapik anadolu muslumani</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>yaratılışa aykırı olan her düşünce yaşadığımız...</td>\n",
       "      <td>0.280237</td>\n",
       "      <td>0.052573</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.069194</td>\n",
       "      <td>0.070871</td>\n",
       "      <td>0.079760</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>leonarda da vinci eşcinsel ve veganmış çağımız...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.093371</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.141656</td>\n",
       "      <td>0.142256</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>ahmet yildiza adalet   temmuz  tarihinde babas...</td>\n",
       "      <td>0.062337</td>\n",
       "      <td>0.116945</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>ailede birinin eşcinsel doğması gerekliydi ve ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.117383</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.178086</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.192285</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>jeandpardaillan el hareketi günümüzde kabul ed...</td>\n",
       "      <td>0.166390</td>\n",
       "      <td>0.078037</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.205417</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.237788</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1001 rows × 1002 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Tweets        ol  eşcinsel  \\\n",
       "0                                  labirsev kargo kadın  0.000000  0.000000   \n",
       "1     elfesyaseher bu tür fotoların yayınlanıp olağa...  0.000000  0.000000   \n",
       "2     beni çocukluğuma götüren kokulardan birisi pla...  0.071309  0.000000   \n",
       "3                                       ana valla sapık  0.000000  0.000000   \n",
       "4                  nocontextsag sapik anadolu muslumani  0.000000  0.000000   \n",
       "...                                                 ...       ...       ...   \n",
       "996   yaratılışa aykırı olan her düşünce yaşadığımız...  0.280237  0.052573   \n",
       "997   leonarda da vinci eşcinsel ve veganmış çağımız...  0.000000  0.093371   \n",
       "998   ahmet yildiza adalet   temmuz  tarihinde babas...  0.062337  0.116945   \n",
       "999   ailede birinin eşcinsel doğması gerekliydi ve ...  0.000000  0.117383   \n",
       "1000  jeandpardaillan el hareketi günümüzde kabul ed...  0.166390  0.078037   \n",
       "\n",
       "         sapık        bu        de       bir        ve        et       ben  \\\n",
       "0     0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1     0.081884  0.092466  0.000000  0.106136  0.000000  0.000000  0.128971   \n",
       "2     0.069566  0.000000  0.000000  0.000000  0.101478  0.000000  0.109569   \n",
       "3     0.231455  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4     0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "996   0.000000  0.000000  0.069194  0.070871  0.079760  0.000000  0.000000   \n",
       "997   0.000000  0.000000  0.000000  0.000000  0.141656  0.142256  0.000000   \n",
       "998   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "999   0.000000  0.000000  0.000000  0.000000  0.178086  0.000000  0.192285   \n",
       "1000  0.000000  0.000000  0.205417  0.000000  0.000000  0.237788  0.000000   \n",
       "\n",
       "      ...  dikkat  cuma  yasak  imkan  mezhep  alev  zafer  yaşam  oktar  \\\n",
       "0     ...     0.0   0.0    0.0    0.0     0.0   0.0    0.0    0.0    0.0   \n",
       "1     ...     0.0   0.0    0.0    0.0     0.0   0.0    0.0    0.0    0.0   \n",
       "2     ...     0.0   0.0    0.0    0.0     0.0   0.0    0.0    0.0    0.0   \n",
       "3     ...     0.0   0.0    0.0    0.0     0.0   0.0    0.0    0.0    0.0   \n",
       "4     ...     0.0   0.0    0.0    0.0     0.0   0.0    0.0    0.0    0.0   \n",
       "...   ...     ...   ...    ...    ...     ...   ...    ...    ...    ...   \n",
       "996   ...     0.0   0.0    0.0    0.0     0.0   0.0    0.0    0.0    0.0   \n",
       "997   ...     0.0   0.0    0.0    0.0     0.0   0.0    0.0    0.0    0.0   \n",
       "998   ...     0.0   0.0    0.0    0.0     0.0   0.0    0.0    0.0    0.0   \n",
       "999   ...     0.0   0.0    0.0    0.0     0.0   0.0    0.0    0.0    0.0   \n",
       "1000  ...     0.0   0.0    0.0    0.0     0.0   0.0    0.0    0.0    0.0   \n",
       "\n",
       "      ciddi  \n",
       "0         0  \n",
       "1         0  \n",
       "2         0  \n",
       "3         0  \n",
       "4         0  \n",
       "...     ...  \n",
       "996       0  \n",
       "997       0  \n",
       "998       0  \n",
       "999       0  \n",
       "1000      0  \n",
       "\n",
       "[1001 rows x 1002 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "526e07dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, 1:]\n",
    "Y = df.loc[:, [\"Tweets\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "88c033a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1e1df4",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "a1e4ab84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "a83b16f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "afd90795",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-13 {color: black;background-color: white;}#sk-container-id-13 pre{padding: 0;}#sk-container-id-13 div.sk-toggleable {background-color: white;}#sk-container-id-13 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-13 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-13 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-13 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-13 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-13 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-13 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-13 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-13 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-13 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-13 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-13 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-13 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-13 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-13 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-13 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-13 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-13 div.sk-item {position: relative;z-index: 1;}#sk-container-id-13 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-13 div.sk-item::before, #sk-container-id-13 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-13 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-13 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-13 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-13 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-13 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-13 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-13 div.sk-label-container {text-align: center;}#sk-container-id-13 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-13 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-13\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" checked><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_model = SVC(kernel='linear')\n",
    "svm_model.fit(X_train, Y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "d5bb7f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = svm_model.predict(X_test)\n",
    "accuracy = accuracy_score(Y_test, Y_pred)\n",
    "f1_scores = f1_score(Y_test, Y_pred, average=\"weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "00dead5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.019933554817275746\n",
      "F1 Score: 0.01882613510520487\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy Score: {accuracy}\\nF1 Score: {f1_scores}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ec5a9a",
   "metadata": {},
   "source": [
    "# WordToVec-CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7241ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "10eefbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1cf57ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_sentences = \"\"\n",
    "\n",
    "for i in df[\"Tweets\"]:\n",
    "    tweets_sentences += i + \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ab44ec9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    " \n",
    "for i in sent_tokenize(tweets_sentences):\n",
    "    temp = []\n",
    "     \n",
    "    # tokenize the sentence into words\n",
    "    for j in word_tokenize(i):\n",
    "        temp.append(j.lower())\n",
    " \n",
    "    data.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8b5d57e6",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-07 16:44:25,908 - gensim.models.word2vec - INFO\n",
      "Msg: collecting all words and their counts\n",
      "\n",
      "2023-06-07 16:44:25,909 - gensim.models.word2vec - INFO\n",
      "Msg: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "\n",
      "2023-06-07 16:44:25,913 - gensim.models.word2vec - INFO\n",
      "Msg: collected 9064 word types from a corpus of 20165 raw words and 1 sentences\n",
      "\n",
      "2023-06-07 16:44:25,914 - gensim.models.word2vec - INFO\n",
      "Msg: Creating a fresh vocabulary\n",
      "\n",
      "2023-06-07 16:44:25,936 - gensim.utils - INFO\n",
      "Msg: Word2Vec lifecycle event {'msg': 'effective_min_count=1 retains 9064 unique words (100.00% of original 9064, drops 0)', 'datetime': '2023-06-07T16:44:25.936901', 'gensim': '4.3.1', 'python': '3.10.11 (tags/v3.10.11:7d4cc5a, Apr  5 2023, 00:38:17) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'prepare_vocab'}\n",
      "\n",
      "2023-06-07 16:44:25,937 - gensim.utils - INFO\n",
      "Msg: Word2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 20165 word corpus (100.00% of original 20165, drops 0)', 'datetime': '2023-06-07T16:44:25.937907', 'gensim': '4.3.1', 'python': '3.10.11 (tags/v3.10.11:7d4cc5a, Apr  5 2023, 00:38:17) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'prepare_vocab'}\n",
      "\n",
      "2023-06-07 16:44:25,974 - gensim.models.word2vec - INFO\n",
      "Msg: deleting the raw counts dictionary of 9064 items\n",
      "\n",
      "2023-06-07 16:44:25,976 - gensim.models.word2vec - INFO\n",
      "Msg: sample=0.001 downsamples 24 most-common words\n",
      "\n",
      "2023-06-07 16:44:25,976 - gensim.utils - INFO\n",
      "Msg: Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 18516.53218989117 word corpus (91.8%% of prior 20165)', 'datetime': '2023-06-07T16:44:25.976133', 'gensim': '4.3.1', 'python': '3.10.11 (tags/v3.10.11:7d4cc5a, Apr  5 2023, 00:38:17) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'prepare_vocab'}\n",
      "\n",
      "2023-06-07 16:44:26,029 - gensim.models.word2vec - INFO\n",
      "Msg: estimated required memory for 9064 words and 100 dimensions: 11783200 bytes\n",
      "\n",
      "2023-06-07 16:44:26,029 - gensim.models.word2vec - INFO\n",
      "Msg: resetting layer weights\n",
      "\n",
      "2023-06-07 16:44:26,033 - gensim.utils - INFO\n",
      "Msg: Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2023-06-07T16:44:26.033673', 'gensim': '4.3.1', 'python': '3.10.11 (tags/v3.10.11:7d4cc5a, Apr  5 2023, 00:38:17) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'build_vocab'}\n",
      "\n",
      "2023-06-07 16:44:26,034 - gensim.utils - INFO\n",
      "Msg: Word2Vec lifecycle event {'msg': 'training model with 3 workers on 9064 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2023-06-07T16:44:26.034688', 'gensim': '4.3.1', 'python': '3.10.11 (tags/v3.10.11:7d4cc5a, Apr  5 2023, 00:38:17) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'train'}\n",
      "\n",
      "2023-06-07 16:44:26,052 - gensim.models.word2vec - INFO\n",
      "Msg: EPOCH 0: training on 20165 raw words (10000 effective words) took 0.0s, 672540 effective words/s\n",
      "\n",
      "2023-06-07 16:44:26,067 - gensim.models.word2vec - INFO\n",
      "Msg: EPOCH 1: training on 20165 raw words (10000 effective words) took 0.0s, 788445 effective words/s\n",
      "\n",
      "2023-06-07 16:44:26,082 - gensim.models.word2vec - INFO\n",
      "Msg: EPOCH 2: training on 20165 raw words (10000 effective words) took 0.0s, 818009 effective words/s\n",
      "\n",
      "2023-06-07 16:44:26,096 - gensim.models.word2vec - INFO\n",
      "Msg: EPOCH 3: training on 20165 raw words (10000 effective words) took 0.0s, 931758 effective words/s\n",
      "\n",
      "2023-06-07 16:44:26,111 - gensim.models.word2vec - INFO\n",
      "Msg: EPOCH 4: training on 20165 raw words (10000 effective words) took 0.0s, 814206 effective words/s\n",
      "\n",
      "2023-06-07 16:44:26,111 - gensim.utils - INFO\n",
      "Msg: Word2Vec lifecycle event {'msg': 'training on 100825 raw words (50000 effective words) took 0.1s, 645255 effective words/s', 'datetime': '2023-06-07T16:44:26.111686', 'gensim': '4.3.1', 'python': '3.10.11 (tags/v3.10.11:7d4cc5a, Apr  5 2023, 00:38:17) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'train'}\n",
      "\n",
      "2023-06-07 16:44:26,112 - gensim.utils - INFO\n",
      "Msg: Word2Vec lifecycle event {'params': 'Word2Vec<vocab=9064, vector_size=100, alpha=0.025>', 'datetime': '2023-06-07T16:44:26.112790', 'gensim': '4.3.1', 'python': '3.10.11 (tags/v3.10.11:7d4cc5a, Apr  5 2023, 00:38:17) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'created'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Create Word2Vec-CBOW\n",
    "wordToVec_cbow_model = gensim.models.Word2Vec(data, min_count = 1, vector_size = 100, window = 5, sg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "920eba62",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_words = wordToVec_cbow_model.wv.most_similar(\"fotoların\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5a02d00c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('çekicek', 0.38024553656578064),\n",
       " ('vekilleri', 0.35367411375045776),\n",
       " ('pilotları', 0.3501938283443451),\n",
       " ('basın', 0.33198702335357666),\n",
       " ('doga', 0.3239491283893585),\n",
       " ('zanaat', 0.31868666410446167),\n",
       " ('sonrasını', 0.3117790222167969),\n",
       " ('guneselifiniz', 0.3080103397369385),\n",
       " ('weirdo', 0.30511805415153503),\n",
       " ('evlilikleri', 0.3033685088157654)]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62c516d",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c71d58d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "59635d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, 1:]\n",
    "Y = df.loc[:, [\"Tweets\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "982949de",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "28007218",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "7780a143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;background-color: white;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" checked><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier()"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decision_tree.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "0038cec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = decision_tree.predict(X_test)\n",
    "accuracy = accuracy_score(Y_test, Y_pred)\n",
    "f1_scores = f1_score(Y_test, Y_pred, average=\"weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "aab66faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.013289036544850499\n",
      "F1 Score: 0.012181616832779622\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy Score: {accuracy}\\nF1 Score: {f1_scores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2674bf29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
